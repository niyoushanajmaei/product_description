{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"train-neo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J0i5MRP0SV8D"},"source":["This notebook uses [GPTNeo](https://github.com/EleutherAI/GPTNeo) by [EleutherAI](eleuther.ai) to fine tune the model and predict multiple files."]},{"cell_type":"markdown","metadata":{"id":"fDBAC9FHmQrf"},"source":["#Fine tune\n","To fine-tune the model copy the excel file containg the labled dataset to \"train/raw\" folder on the drive"]},{"cell_type":"markdown","metadata":{"id":"PK7uCHSo40Zh"},"source":["Choose the following options:\n","1. re-initialize this configuration [1]\n","2. the google account with the cloud storage [1]\n","3. gpt project [10]\n","4. No [n]"]},{"cell_type":"code","metadata":{"id":"xFsSiQe54zN0"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","#!gcloud auth login\n","!gcloud init"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bryzzTww6DIr"},"source":["Mount the drive where the generated predictions will be stored."]},{"cell_type":"code","metadata":{"id":"mbBqicS24-rQ"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQDvpLNdJvlu"},"source":["!mkdir /content/drive/MyDrive/train/train_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np-rFmY4mzZu"},"source":["import pandas as pd\n","from pandas import read_excel\n","from pathlib import Path\n","import numpy as np\n","import lxml.html\n","import string\n","import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJbfFD_6m3sO"},"source":["def read_batch(read_dir):\n","    dfs = []\n","    c=0\n","    for path in os.listdir(read_dir):\n","        full_path = os.path.join(read_dir, path)\n","        if os.path.isfile(full_path):\n","            dfs.append(read_csv(full_path))\n","            c+=1\n","            print(\"read file #\"+ str(c)) \n","\n","    df = pd.concat(dfs)\n","    #print(len(df.index))\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def read_csv(path):\n","    #file_name = '02_batch_import_Dior.xlsx'\n","    #df = read_excel(Path(path,file_name), sheet_name = my_sheet,keep_default_na=False)\n","    df = pd.read_excel(path, sheet_name='Sheet1')\n","    df.reset_index(drop=True, inplace=True)\n","    return clean(df)\n","\n","def clean(df):\n","    to_keep=[\"brand\",\"category\",\"description\",\"color\",\"gender\",\"pattern\",\"neckline\",\"sleeves\",\"material\"]\n","    to_drop=[]\n","    for col in df.columns:\n","        if col not in to_keep:\n","            to_drop.append(col)\n","    df.drop(to_drop, inplace=True, axis=1)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjwIIkz_nCLZ"},"source":["def write_as_txt(df,write_dir):\n","   # Shuffle indices and split the data Train 70%, val 30%\n","   df = df.iloc[np.random.permutation(len(df))]\n","   #train, validate= np.split(df.sample(frac=1), [int(.7*len(df))])\n","   train = df\n","   write(train, \"train\",write_dir)\n","   #write(validate,\"validate\",write_dir)\n","   #write(test,\"test\",write_dir)\n","\n","def write(df, t,write_dir):\n","    dir = write_dir\n","    if t==\"train\":\n","        path = dir+\"train_text/\"\n","    elif t==\"validate\":\n","        path = dir+\"val_text/\"\n","    #elif t==\"test\":\n","    #    path = dir+\"test_text/\"\n","    #ref_path = dir+\"test_text_ref/\"\n","    c=0\n","    data= df.to_dict('index')\n","    #if t == \"test\":\n","    #    #write the test set with and without the lables to have a reference\n","    #    for k,value in data.items():\n","    #        value = {k:v for k,v in value.items() if str(v)!= '' and str(v).strip() != '' and str(v)!='nan' and str(v)!='null'}\n","    #        write_dict(value,ref_path+\"product\"+str(c)+\".txt\",\"n\")\n","    #        c+=1\n","    #    c=0\n","    #    for k,value in data.items():\n","    #        value = {k:v for k,v in value.items() if str(v)!= '' and str(v).strip() != '' and str(v)!='nan' and str(v)!='null'}\n","    #        write_dict(value,path+\"product\"+str(c)+\".txt\",\"t\")\n","    #        c+=1 \n","    #else :\n","    for k,value in data.items():\n","        value = {k:v for k,v in value.items() if str(v)!= '' and str(v).strip() != '' and str(v)!='nan' and str(v)!='null' and str(v)!=  '[]'}\n","        write_dict(value,path+\"product\"+str(c)+\".txt\",\"n\")\n","        c+=1\n","    print(\"writing \"+t+ \" successful\")\n","\n","# writes the file with format:\n","# when type in \"n\" for normal\n","# {\"tag1\" : \"value1\", \"tag2\": \"value2\", ....} \\n description: \"description_en\" \\n ### \\n\n","# when type is \"t\" for test\n","# {\"tag1\" : \"value1\", \"tag2\": \"value2\", ....} \\n description: \n","def write_dict(dict, path, type):\n","    feat,desc = get_data(dict) # only write the features and the generated descriptions\n","    if (desc):\n","        with open(path, 'w') as f:\n","            txt = feat + \"\\n\"\n","            if type == \"n\":\n","                if txt[-1] != '\\n':\n","                    txt+='\\n'\n","                txt += \"description: \" + desc + \"\\n###\\n\"\n","            txt = clean_txt(txt)\n","            print(txt,file =f)\n","            \n","def clean_txt(st):\n","    st = st.strip()\n","    st = st.replace('\"','')\n","    st = st.replace('[','')\n","    st = st.replace(']','')\n","    st = st.replace(\"'\",'')\n","    return st\n","\n","def get_data(dict):\n","    feat = dict\n","    desc = dict.pop(\"description\", None)\n","    feat = str(feat).replace('{','').replace('}','').replace(\"'\",'')\n","    return feat,desc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92fhAh7emuPY"},"source":["read_dir = \"/content/drive/MyDrive/train/raw/\"\n","write_dir = \"/content/drive/MyDrive/train/\"\n","write_as_txt(read_batch(read_dir),write_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD2WROApnpkA"},"source":["!cat /content/drive/MyDrive/train/train_text/*.txt > /content/drive/MyDrive/train/concat.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-53qkZV6Lv9"},"source":["import os\n","%tensorflow_version 2.x\n","!git clone https://github.com/EleutherAI/gpt-neo\n","%cd gpt-neo\n","!pip3 install -q -r requirements.txt\n","pretrained_model = None\n","dataset = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"wid6pXwlYuIN"},"source":["!pip install -U tensorflow-gcs-config==2.1.3\n","!pip install -q t5 tensorflow-text==2.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cr_c6A2NBK5i"},"source":["path_to_cloud_bucket = 'gs://test-gpt-j/' "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgSYepSMO8YJ"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"pM8jP3Am_hsx"},"source":["# Select a Dataset:\n","import os\n","dataset_path = \"/content/drive/MyDrive/train/concat/\"\n","dataset_name = 'dataset_name'\n","out_name = dataset_name + \"_tokenized\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"both","id":"Pq5u0WUSJWwz"},"source":["# Tokenize Data\n","!python data/create_tfrecords.py --input_dir $dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1\n","\n","# copy the data to your bucket\n","if not path_to_cloud_bucket.endswith('/'):\n","  path_to_cloud_bucket += '/'\n","copy_loc = path_to_cloud_bucket + \"dataset/\"\n","!gsutil -m cp -r /content/gpt-neo/$out_name $copy_loc\n","!gsutil ls $path_to_cloud_bucket"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUY6IWIXPU7E"},"source":["# Configs\n","dataset configs\n","If dataset_name was changed, change the name of the file being written accordingly."]},{"cell_type":"code","metadata":{"id":"MCsZP48vavCP"},"source":["%%writefile configs/dataset_configs/dataset_name.json\n","\n","{\n","  \"path\": \"gs://test-gpt-j/dataset/dataset_name_*.tfrecords\",\n","  \"eval_path\": \"\",\n","  \"n_vocab\": 50256,\n","  \"tokenizer_is_pretrained\": true,\n","  \"tokenizer_path\": \"gpt2\",\n","  \"eos_id\": 50256,\n","  \"padding_id\": 50257\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FK4Sfh9GPdAk"},"source":["Model configs"]},{"cell_type":"markdown","metadata":{"id":"PyiD6ZlSpF1A"},"source":["If dataset_name was changed, change the value for \"dataset\" in the config accordingly."]},{"cell_type":"code","metadata":{"id":"L9hUDdokiWj6"},"source":["%%writefile configs/GPT3_XL.json\n","\n","{\n","    \"n_head\": 16,\n","    \"n_vocab\": 50257,\n","    \"embed_dropout\": 0,\n","    \"lr\": 0.0002,\n","    \"lr_decay\": \"cosine\",\n","    \"warmup_steps\": 3000,\n","    \"beta1\": 0.9,\n","    \"beta2\": 0.95,\n","    \"epsilon\": 1e-8,\n","    \"opt_name\": \"adam\",\n","    \"weight_decay\": 0,\n","    \"train_batch_size\": 256,\n","    \"attn_dropout\": 0,\n","    \"train_steps\": 600000,\n","    \"eval_steps\": 0,\n","    \"predict_steps\": 1,\n","    \"res_dropout\": 0,\n","    \"eval_batch_size\": 4,\n","    \"predict_batch_size\": 1,\n","    \"iterations\": 100,\n","    \"n_embd\": 2048,\n","    \"datasets\": [[\"dataset_name\", null, null, null]],\n","    \"model\": \"GPT\",\n","    \"model_path\": \"gs://test-gpt-j/\",\n","    \"n_ctx\": 2048,\n","    \"n_layer\": 24,\n","    \"scale_by_depth\": true,\n","    \"scale_by_in\": false,\n","    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n","    \"mesh_shape\": \"x:4,y:2\",\n","    \"layout\": \"intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y\",\n","    \"activation_function\": \"gelu\",\n","    \"recompute_grad\": true,\n","    \"gradient_clipping\": 1.0,\n","    \"tokens_per_mb_per_replica\": 2048,\n","    \"precision\": \"bfloat16\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"koKQHA5ikCvD"},"source":["#Pretrained Model"]},{"cell_type":"code","metadata":{"id":"Eu0sN0SEs7n9"},"source":["pretrained_model = 'GPT3_XL' \n","path_to_local_weights = f\"/content/gpt-neo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgTG1ammqGB0"},"source":["pretrained_model = 'GPT3_XL' \n","!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\"\n","path_to_local_weights = f\"/content/gpt-neo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GU3BDNJN_ZXE"},"source":["\n","bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n","!gsutil -m cp -r $path_to_local_weights $bucket_base"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRJMOaRepmZq"},"source":["If dataset_name was modified, change teh value of \"dataset\" in \"mods\" accordingly."]},{"cell_type":"code","metadata":{"id":"Laf0slBMDCUj"},"source":["import json\n","from pprint import pprint\n","\n","path_to_model = \"\" \n","batch_size = 8 \n","dset = \"prod_desc_gpt_j\"  \n","mesh_shape = \"x:4,y:2\"\n","train_steps = 1000 \n","steps_per_checkpoint = 500 \n","start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n","\n","if path_to_model == \"\":\n","  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n","print(f'MODEL PATH: {path_to_model}\\n')\n","\n","if dset == \"\" and dataset != \"Sampling_Only\":\n","  dset = dataset\n","elif dataset is None and dset == \"\":\n","  dset = \"pile\"\n","\n","def pad_to_multiple_of(n, mult):\n","  \"\"\"\n","  pads n to a multiple of mult\n","  \"\"\"\n","  extra = n % mult\n","  if extra > 0:\n","      n = n + mult - extra\n","  return n\n","\n","with open(f'{path_to_local_weights}/config.json', 'r') as f:\n","  data = json.load(f)\n","  pprint(data)\n","  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n","  mods = {\n","          \"mesh_shape\": mesh_shape,\n","          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n","          \"model_path\": path_to_model,\n","          \"datasets\": [[\"dataset_name\", None, None, None]],\n","          \"train_steps\": start_step + train_steps,\n","          \"eval_steps\": 0,\n","          \"train_batch_size\": batch_size,\n","          \"predict_batch_size\": batch_size\n","        }\n","  data.update(mods)\n","  print('\\n--->\\n')\n","  pprint(data)\n","  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n","    json.dump(data, outfile, indent=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPwwbPCA6O7r"},"source":["### Begin Fine-Tuning\n"]},{"cell_type":"markdown","metadata":{"id":"gmT_2wZdyPPF"},"source":["Give permissions to the service by adding it as a storage admin of the bucket if permission denied errors rise."]},{"cell_type":"code","metadata":{"id":"0YlaHzyXuMaj"},"source":["!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pm363l-UuEgP"},"source":["#Evaluate the model"]},{"cell_type":"markdown","metadata":{"id":"vYnDABh8u-xu"},"source":["If you want to evaluate the model, you have to make another concat.txt file, tokenize it and upload to the bucket as before and run the model on eval mode."]},{"cell_type":"code","metadata":{"id":"Ypr8waVSuHFy"},"source":["# Select a Dataset:\n","import os\n","dataset_path = \"/content/drive/MyDrive/eval_dataset/\"\n","dataset_name = 'eval_dataset'\n","out_name = dataset_name + \"_tokenized\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EObZPRG4ucH_"},"source":["# Tokenize Data\n","!python data/create_tfrecords.py --input_dir $dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1\n","\n","# copy the data to your bucket\n","if not path_to_cloud_bucket.endswith('/'):\n","  path_to_cloud_bucket += '/'\n","copy_loc = path_to_cloud_bucket + \"eval_datasets/\"\n","!gsutil -m cp -r /content/gpt-neo/$out_name $copy_loc\n","!gsutil ls $path_to_cloud_bucket"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZPQ4LH8u8S5"},"source":["create dataset configs"]},{"cell_type":"code","metadata":{"id":"aAx8Rw9LunHr"},"source":["%%writefile configs/dataset_configs/eval_dataset.json\n","\n","{\n","  \"path\": \"\",\n","  \"eval_path\": \"gs://test-gpt-j/eval_datasets/*.tfrecords\",\n","  \"n_vocab\": 50256,\n","  \"tokenizer_is_pretrained\": true,\n","  \"tokenizer_path\": \"gpt2\",\n","  \"eos_id\": 50256,\n","  \"padding_id\": 50257\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dQ5dbRdau4NQ"},"source":["update the configs to point to the dataset"]},{"cell_type":"code","metadata":{"id":"bBQGGK5gutZk"},"source":["import json\n","from pprint import pprint\n","\n","batch_size = 8\n","assert pretrained_model is not None\n","with open(f'configs/{pretrained_model}.json', 'r') as f:\n","  data = json.load(f)\n","  pprint(data)\n","  dset_val = [[\"eval_dataset\", None, None, None]]\n","  mods = {\n","          \"datasets\": dset_val,\n","          \"eval_steps\": 139 // batch_size,\n","          \"train_batch_size\": batch_size,\n","          \"eval_batch_size\": batch_size,\n","        }\n","  data.update(mods)\n","  print('\\n--->\\n')\n","  pprint(data)\n","  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n","    json.dump(data, outfile, indent=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bF0Wh8ARvFlT"},"source":["Run evaluation"]},{"cell_type":"code","metadata":{"id":"KRGI8BIyvEoz"},"source":["!python3 main.py --eval --tpu colab --model $pretrained_model"],"execution_count":null,"outputs":[]}]}