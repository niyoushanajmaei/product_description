{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"generate-neo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J0i5MRP0SV8D"},"source":["This notebook uses [GPTNeo](https://github.com/EleutherAI/GPTNeo) by [EleutherAI](eleuther.ai) to fine tune the model and predict a batch of instances."]},{"cell_type":"markdown","metadata":{"id":"zoGedGqhbble"},"source":["# Product Description Generation\n","\n","If a new batch is being generated: \n","\n","1. Make sure you have prepared the dataset with the \"prepare\" notebook\n","\n","2. Make sure the fine tuned model is uploaded to the bucket\n"]},{"cell_type":"markdown","metadata":{"id":"-5HYR_tf3MIG"},"source":["Choose the following options:\n","1. re-initialize this configuration [1]\n","2. the google account with the cloud storage [1]\n","3. gpt project [10]\n","4. No [n]"]},{"cell_type":"code","metadata":{"id":"2VNod-J73IKL"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","#!gcloud auth login\n","!gcloud init"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Jmdi1Ol30Ee"},"source":["Mount the drive with the excel files where also the generated descriptions will be stored.\n"]},{"cell_type":"code","metadata":{"id":"vthqcBXs3dYG"},"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-53qkZV6Lv9"},"source":["import os\n","%tensorflow_version 2.x\n","!git clone https://github.com/EleutherAI/gpt-neo\n","%cd gpt-neo\n","!pip3 install -q -r requirements.txt\n","pretrained_model = None\n","dataset = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wid6pXwlYuIN"},"source":["!pip install -U tensorflow-gcs-config==2.1.3\n","!pip install -q t5 tensorflow-text==2.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cr_c6A2NBK5i"},"source":["path_to_cloud_bucket = 'gs://test-gpt-j/' "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUY6IWIXPU7E"},"source":["# Configs\n","dataset configs"]},{"cell_type":"code","metadata":{"id":"MCsZP48vavCP"},"source":["%%writefile configs/dataset_configs/prod_desc_gpt_j.json\n","\n","{\n","  \"path\": \"gs://test-gpt-j/datasets/prod_desc_gpt_j_*.tfrecords\",\n","  \"eval_path\": \"\",\n","  \"n_vocab\": 50256,\n","  \"tokenizer_is_pretrained\": true,\n","  \"tokenizer_path\": \"gpt2\",\n","  \"eos_id\": 50256,\n","  \"padding_id\": 50257\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FK4Sfh9GPdAk"},"source":["Model configs"]},{"cell_type":"code","metadata":{"id":"L9hUDdokiWj6"},"source":["%%writefile configs/GPT3_XL.json\n","\n","{\n","    \"n_head\": 16,\n","    \"n_vocab\": 50257,\n","    \"embed_dropout\": 0,\n","    \"lr\": 0.0002,\n","    \"lr_decay\": \"cosine\",\n","    \"warmup_steps\": 3000,\n","    \"beta1\": 0.9,\n","    \"beta2\": 0.95,\n","    \"epsilon\": 1e-8,\n","    \"opt_name\": \"adam\",\n","    \"weight_decay\": 0,\n","    \"train_batch_size\": 256,\n","    \"attn_dropout\": 0,\n","    \"train_steps\": 600000,\n","    \"eval_steps\": 0,\n","    \"predict_steps\": 1,\n","    \"res_dropout\": 0,\n","    \"eval_batch_size\": 4,\n","    \"predict_batch_size\": 1,\n","    \"iterations\": 100,\n","    \"n_embd\": 2048,\n","    \"datasets\": [[\"prod_desc_gpt_j\", null, null, null]],\n","    \"model\": \"GPT\",\n","    \"model_path\": \"gs://test-gpt-j/\",\n","    \"n_ctx\": 2048,\n","    \"n_layer\": 24,\n","    \"scale_by_depth\": true,\n","    \"scale_by_in\": false,\n","    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n","    \"mesh_shape\": \"x:4,y:2\",\n","    \"layout\": \"intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y\",\n","    \"activation_function\": \"gelu\",\n","    \"recompute_grad\": true,\n","    \"gradient_clipping\": 1.0,\n","    \"tokens_per_mb_per_replica\": 2048,\n","    \"precision\": \"bfloat16\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"koKQHA5ikCvD"},"source":["#Fine tuned model"]},{"cell_type":"code","metadata":{"id":"GU3BDNJN_ZXE"},"source":["bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n","pretrained_model = 'GPT3_XL'\n","!mkdir pretrained\n","!gsutil -m cp gs://test-gpt-j/GPT3_XL/config.json pretrained\n","path_to_local_weights = f\"/content/gpt-neo/pretrained/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Laf0slBMDCUj"},"source":["import json\n","from pprint import pprint\n","\n","path_to_model = \"\" \n","batch_size = 8 \n","dset = \"prod_desc_gpt_j\"  \n","mesh_shape = \"x:4,y:2\"\n","train_steps = 1000 \n","steps_per_checkpoint = 500 \n","start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n","\n","if path_to_model == \"\":\n","  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n","print(f'MODEL PATH: {path_to_model}\\n')\n","\n","if dset == \"\" and dataset != \"Sampling_Only\":\n","  dset = dataset\n","elif dataset is None and dset == \"\":\n","  dset = \"pile\"\n","\n","def pad_to_multiple_of(n, mult):\n","  \"\"\"\n","  pads n to a multiple of mult\n","  \"\"\"\n","  extra = n % mult\n","  if extra > 0:\n","      n = n + mult - extra\n","  return n\n","\n","with open(f'{path_to_local_weights}config.json', 'r') as f:\n","  data = json.load(f)\n","  pprint(data)\n","  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n","  mods = {\n","          \"mesh_shape\": mesh_shape,\n","          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n","          \"model_path\": path_to_model,\n","          \"datasets\": dset_val,\n","          \"train_steps\": start_step + train_steps,\n","          \"eval_steps\": 0,\n","          \"train_batch_size\": batch_size,\n","          \"predict_batch_size\": batch_size\n","        }\n","  data.update(mods)\n","  print('\\n--->\\n')\n","  pprint(data)\n","  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n","    json.dump(data, outfile, indent=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_HxtEmBGTGT"},"source":["### Sample from your model\n","\n","Once the pretrained model (fine tuned) is in the bucket, sample from it."]},{"cell_type":"code","metadata":{"id":"OLPyuWz_j1q9"},"source":["%cd ..\n","!mkdir drive/MyDrive/dataset/gen/\n","%cd gpt-neo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krwBdj-sQXBz"},"source":["Copy the test set to gpt-neo/test/"]},{"cell_type":"code","metadata":{"id":"EC5uFTMbCSJR"},"source":["from data.encoders import encode\n","from functools import partial\n","import mesh_tensorflow as mtf\n","import tensorflow.compat.v1 as tf\n","from tensorflow.python.tpu import tpu_config, tpu_estimator\n","from tensorflow_estimator.python.estimator import estimator as estimator_lib\n","from utils import save_config, expand_attention_types_params, yes_or_no, remove_gs_or_filepath, setup_logging, \\\n","    check_dataset\n","from inputs import sequential_input, mlm_sample_text, generic_text\n","from export import export_model\n","from model_fns import model_fn\n","from data.encoders import fetch_encoder\n","from configs import fetch_model_params\n","from tasks import task_descriptors\n","import argparse\n","import json\n","import numpy as np\n","import gc\n","import sys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE4AmPd8Hdyo"},"source":["def pred_input(params, enc=None,\n","               path_to_prompt=\"\"):\n","    unicorns = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n","               \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n","               \"researchers was the fact that the unicorns spoke perfect English.\"\n","\n","    text = unicorns if path_to_prompt == \"\" else open(path_to_prompt, \"r\").read()\n","    tokens = encode(enc, text)\n","\n","    if len(tokens) > params[\"n_ctx\"]:\n","        tokens = tokens[len(tokens) - params[\"n_ctx\"]:]\n","    if len(tokens) < params[\"n_ctx\"]:\n","        tokens = tf.pad(tokens, [[0, params[\"n_ctx\"] - len(tokens)]], constant_values=params[\"padding_id\"])\n","\n","    t = tf.broadcast_to(tokens, [params[\"batch_size\"], params[\"n_ctx\"]])\n","    dataset = tf.data.Dataset.from_tensors(t)\n","\n","    def _dummy_labels(x):\n","        return x, x\n","        \n","    del t\n","    del tokens\n","    gc.collect()\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pj3CFswIHCSf"},"source":["def handle_pred_output(predictions, enc, params, out_name=\"test\"):\n","    with tf.gfile.Open(out_name, \"w\") as f:\n","        for i, p in enumerate(predictions):\n","            p = p[\"outputs\"]\n","            # remove eos + padding ids from output\n","            idx = np.argmax(p == params['eos_id'])\n","            if idx > 0:\n","                p = p[:idx]\n","            idx = np.argmax(p == params['padding_id'])\n","            if idx > 0:\n","                p = p[:idx]\n","            text = enc.decode(p)\n","            f.write(text)\n","            #only using the first prediction\n","            break\n","\n","    return \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Bz745eU6fRu"},"source":["def infer(path,name):\n","    tf.disable_v2_behavior()\n","\n","    tpu= \"colab\"\n","    model= pretrained_model \n","    steps_per_checkpoint = 500 \n","\n","    # Read params of model\n","    params = fetch_model_params(model)\n","\n","    # Fetch appropriate input functions\n","    input_fn = params.get(\"input_fn\", \"sequential_input\")\n","    if input_fn == \"sequential_input\":\n","        input_fn = sequential_input\n","    elif input_fn == \"generic_text\":\n","        input_fn = generic_text\n","    pred_input_fn = pred_input\n","    handle_pred_output_fn = handle_pred_output\n","\n","    # get current step\n","    current_step = int(estimator_lib._load_global_step_from_checkpoint_dir(params[\"model_path\"]))\n","    \n","    if params[\"mlm_training\"]:\n","        mlm_sample_text_fn = partial(mlm_sample_text, params)\n","        input_fn = partial(generic_text, sample_text_fn=mlm_sample_text_fn)\n","        if args.check_dataset:\n","            check_dataset(input_fn, params)\n","\n","\n","    # Fetch encoder per params\n","    encoder = fetch_encoder(params)\n","\n","    pred_input_fn = partial(pred_input_fn, path_to_prompt=path, enc=encoder)\n","\n","    # Save config to logdir for experiment management\n","    save_config(params, params[\"model_path\"])\n","\n","    # Add to params: auto_layout, auto_layout_and_mesh_shape, use_tpu, num_cores\n","    mesh_shape = mtf.convert_to_shape(params[\"mesh_shape\"])\n","    params[\"num_cores\"] = mesh_shape.size\n","    params[\"auto_layout\"] = True\n","    params[\"auto_layout_and_mesh_shape\"] = True\n","    params[\"use_tpu\"] = True \n","    params[\"gpu_ids\"] = None\n","    params[\"steps_per_checkpoint\"] = steps_per_checkpoint\n","    # Expand attention types param\n","    params[\"attention_types\"] = expand_attention_types_params(params[\"attention_types\"])\n","    assert len(params[\"attention_types\"]) == params[\"n_layer\"]  # Assert that the length of expanded list = num layers\n","    params[\"predict_batch_size\"] = params.get(\"predict_batch_size\", 1)  # Default to 1\n","    params[\"predict\"] = True\n","    params['model'] = params.get(\"model\", \"GPT\") # Default model selection to GPT since it's the only option for now\n","    params[\"export\"] = False\n","    # Set sampling parameters\n","    params[\"sampling_use_entmax\"] = False\n","\n","    # Sample quality of MoE models suffers when using the faster sampling method, so default to slow_sampling if\n","    # moe layers are present\n","    params[\"slow_sampling\"] = True if params[\"moe_layers\"] is not None else False\n","\n","    #logger.info(f\"params = {params}\")\n","\n","    # Get eval tasks from params\n","    eval_tasks = params.get(\"eval_tasks\", [])\n","    has_predict_or_eval_steps_or_eval_tasks = params[\"predict_steps\"] > 0 or params[\"eval_steps\"] > 0 or len(\n","        eval_tasks) > 0\n","\n","    for t in eval_tasks:\n","        assert t in task_descriptors, f\"Eval task '{t}' is not known\"\n","        task_descriptors[t][\"init_fn\"](params)\n","\n","    # Set up TPUs and Estimator\n","    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() if params[\"use_tpu\"] else None\n","    \n","    config = tpu_config.RunConfig(\n","        cluster=tpu_cluster_resolver,\n","        model_dir=params[\"model_path\"],\n","        save_checkpoints_steps=None,  # Disable the default saver\n","        save_checkpoints_secs=None,  # Disable the default saver\n","        log_step_count_steps=params[\"iterations\"],\n","        save_summary_steps=params[\"iterations\"],\n","        tpu_config=tpu_config.TPUConfig(\n","            num_shards=mesh_shape.size,\n","            iterations_per_loop=params[\"iterations\"],\n","            num_cores_per_replica=1,\n","            per_host_input_for_training=tpu_config.InputPipelineConfig.BROADCAST))\n","\n","    estimator = tpu_estimator.TPUEstimator(\n","        use_tpu=params[\"use_tpu\"],\n","        model_fn=model_fn,\n","        config=config,\n","        train_batch_size=params[\"train_batch_size\"],\n","        eval_batch_size=params[\"train_batch_size\"],\n","        predict_batch_size=params[\"predict_batch_size\"],\n","        params=params)\n","\n","    def _make_task_estimator(task):\n","        task_params = params.copy()\n","        task_params[\"eval_task\"] = task\n","        return tpu_estimator.TPUEstimator(\n","            use_tpu=params[\"use_tpu\"],\n","            model_fn=model_fn,\n","            config=config,\n","            train_batch_size=params[\"train_batch_size\"],\n","            eval_batch_size=params[\"eval_batch_size\"],\n","            predict_batch_size=params[\"predict_batch_size\"],\n","            params=task_params)\n","\n","    predictions = estimator.predict(input_fn=pred_input_fn)\n","\n","    #logger.info(\"Predictions generated\")\n","    enc = fetch_encoder(params)\n","    out = \"/content/drive/MyDrive/dataset/gen/\"+name\n","    handle_pred_output(predictions, enc, params, out_name=out)\n","\n","    del predictions\n","    del estimator\n","    del enc\n","    del current_step\n","    del mesh_shape\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    tf.reset_default_graph()\n","\n","    return\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8jffnKBdXoU"},"source":["def infer_all(dir):\n","  to_be_gen = []\n","  generated = []\n","  with open(\"/content/drive/MyDrive/dataset/checkpoint.txt\",\"r\") as f:\n","    generated = f.read().split('\\n')\n","  for path in os.listdir(dir):\n","    full_path = os.path.join(dir, path)\n","    if os.path.isfile(full_path):\n","      if path not in generated:\n","        to_be_gen.append(path)\n","      \n","  c=0\n","  for path in to_be_gen:\n","    full_path = dir + path\n","    infer(full_path,path)\n","    with open(\"/content/drive/MyDrive/dataset/checkpoint.txt\",\"a\") as f:\n","      f.write(f\"{path}\\n\")\n","    c+=1\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jlnMUPlisjT"},"source":["import time\n","start = time.time()\n","infer_all(\"/content/drive/MyDrive/dataset/test/\")\n","print(f\"All done in {time.time()-start}s\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0l0dr_721x2"},"source":["##Warning\n","The results will be deleted from the drive upon running the model on another dataset."]}]}